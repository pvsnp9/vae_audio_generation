{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import Tuple, Callable\n",
    "from hyperparams import TVAEParams\n",
    "from tvae import TVAE\n",
    "from dataclasses import asdict\n",
    "from earlystopping import EarlyStopping\n",
    "from utils import *\n",
    "from raw_audio_dataloader import get_tvae_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion is \n",
    "# (reconstruction, target, mu, logvar) -> (total_loss, mse_loss, kl_loss)\n",
    "\n",
    "CriterionType = Callable[[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor],Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:TVAE, train_loader: DataLoader, optimzer: optim.Optimizer, criterion:CriterionType, hp:TVAEParams)->float:\n",
    "    model.train()\n",
    "    total_loss = 0.0 \n",
    "    total_mse_loss = 0.0\n",
    "    total_kl_loss = 0.0\n",
    "\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(hp.device), tgt.to(hp.device) # [B, C, Seq]\n",
    "        optimzer.zero_grad()\n",
    "        recon, mu, logvar = model(src, tgt) # recon[B, seq, C]\n",
    "        # permute tgt to [B, Seq, C] to compute loss\n",
    "        tgt = tgt.permute(0, 2 , 1)\n",
    "        loss, mse_loss, kl_loss = criterion(tgt, recon, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "\n",
    "        total_loss += loss.item() * src.size(0)\n",
    "        total_mse_loss += mse_loss.item() * src.size(0)\n",
    "        total_kl_loss += kl_loss.item() * src.size(0)\n",
    "\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    total_mse_loss /= len(train_loader.dataset)\n",
    "    total_kl_loss /= len(train_loader.dataset)\n",
    "\n",
    "    return total_loss, total_mse_loss, total_kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model:TVAE, val_loader: DataLoader, criterion:CriterionType, hp:TVAEParams)->float:\n",
    "    model.eval()\n",
    "    total_loss = 0.0 \n",
    "    total_mse_loss = 0.0\n",
    "    total_kl_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(hp.device), tgt.to(hp.device)\n",
    "            recon, mu, logvar = model(src, tgt)\n",
    "            tgt = tgt.permute(0, 2, 1)\n",
    "\n",
    "            loss, mse_loss, kl_loss = criterion(tgt, recon, mu, logvar)\n",
    "            total_loss += loss.item() * src.size(0)\n",
    "            total_mse_loss += mse_loss.item() * src.size(0)\n",
    "            total_kl_loss += kl_loss.item() * src.size(0)\n",
    "\n",
    "        total_loss /= len(val_loader.dataset)\n",
    "        total_mse_loss /= len(val_loader.dataset)\n",
    "        total_kl_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    return total_loss, total_mse_loss, total_kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = TVAEParams()\n",
    "print(f\"Parameters: {asdict(hp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TVAE(hp=hp).to(hp.device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=hp.lr)\n",
    "criterion = vae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_tvae_dataloaders(hp=hp)\n",
    "print(f'Train Seq #{len(train_loader.dataset)}, Val Seq #{len(val_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=5, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_train:\n",
    "    log_data = []\n",
    "    for epoch in range(hp.num_epochs):\n",
    "        train_loss, train_mse_loss, train_kl_loss = train(model=model, train_loader=train_loader, optimzer=optimizer, criterion=criterion, hp=hp)\n",
    "        val_loss, val_mse_loss, val_kl_loss = validate(model=model, val_loader=val_loader, cirterion=criterion, hp=hp)\n",
    "        print(f\"Train [ LOSS:{train_loss:.3f}, MSE:{train_mse_loss:.3f}, KL{train_kl_loss:.3f}]\")\n",
    "        print(f\"Val [ LOSS:{val_loss:.3f}, MSE:{val_mse_loss:.3f}, KL{val_kl_loss:.3f}]\")\n",
    "\n",
    "        es(val_loss=vae_loss, model=model, model_dir=hp.model_dir, model_file_name=hp.model_file_name)\n",
    "        if es.early_stop:\n",
    "            print(f\"Early stop triggered @ EPOCH: {epoch}\")\n",
    "            break\n",
    "\n",
    "        logs = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_mse_loss\": train_mse_loss,\n",
    "                \"train_kl_loss\": train_kl_loss,\n",
    "                \"train_total_loss\": train_loss,\n",
    "                \"val_mse_loss\": val_mse_loss,\n",
    "                \"val_kl_loss\": val_kl_loss,\n",
    "                \"val_total_loss\": val_loss\n",
    "            }\n",
    "        \n",
    "        log_data.append(log_data)\n",
    "\n",
    "    # Save training logs to a JSON file.\n",
    "    os.makedirs(hp.log_dir, exist_ok=True)\n",
    "    log_path = os.path.join(hp.log_dir, hp.train_log_file)\n",
    "    with open(log_path, 'w') as f:\n",
    "        json.dump(logs, f, indent=4)\n",
    "    print(f\"Training logs saved to {log_path}\")\n",
    "    print(\"Training and validation completed \")\n",
    "else:\n",
    "    model.load_checkpoint(f\"{hp.model_dir}/{hp.model_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_generated_audio(model,hp=hp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
